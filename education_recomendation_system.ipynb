{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKmUt-si8FTH"
      },
      "source": [
        "Load database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AXty9LJ8FPi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zSLFyTL2myAm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "df1 = pd.read_csv(\"datasets/student-scores.csv\")\n",
        "\n",
        "df = df1.copy()\n",
        "#df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni88wPVQ8zWv"
      },
      "source": [
        "Drop irrelevant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TxF63LxK8437"
      },
      "outputs": [],
      "source": [
        "df.columns\n",
        "df.drop(columns=['id','first_name','last_name','email'],axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjPnU16G9AdU"
      },
      "source": [
        "Create new features from all score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ruxeGIed9jqA"
      },
      "outputs": [],
      "source": [
        "df[\"total_score\"] = df[\"math_score\"] + df[\"history_score\"] + df[\"physics_score\"] + df[\"chemistry_score\"] + df[\"biology_score\"] + df[\"english_score\"] + df[\"geography_score\"]\n",
        "df[\"average_score\"] = df[\"total_score\"] / 7\n",
        "#df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "64Ok5lzc9yiJ",
        "outputId": "8f75ee4c-6024-49d5-81bb-f613999c658e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>part_time_job</th>\n",
              "      <th>absence_days</th>\n",
              "      <th>extracurricular_activities</th>\n",
              "      <th>weekly_self_study_hours</th>\n",
              "      <th>career_aspiration</th>\n",
              "      <th>math_score</th>\n",
              "      <th>history_score</th>\n",
              "      <th>physics_score</th>\n",
              "      <th>chemistry_score</th>\n",
              "      <th>biology_score</th>\n",
              "      <th>english_score</th>\n",
              "      <th>geography_score</th>\n",
              "      <th>total_score</th>\n",
              "      <th>average_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>27</td>\n",
              "      <td>Lawyer</td>\n",
              "      <td>73</td>\n",
              "      <td>81</td>\n",
              "      <td>93</td>\n",
              "      <td>97</td>\n",
              "      <td>63</td>\n",
              "      <td>80</td>\n",
              "      <td>87</td>\n",
              "      <td>574</td>\n",
              "      <td>82.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>47</td>\n",
              "      <td>Doctor</td>\n",
              "      <td>90</td>\n",
              "      <td>86</td>\n",
              "      <td>96</td>\n",
              "      <td>100</td>\n",
              "      <td>90</td>\n",
              "      <td>88</td>\n",
              "      <td>90</td>\n",
              "      <td>640</td>\n",
              "      <td>91.428571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>female</td>\n",
              "      <td>False</td>\n",
              "      <td>9</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "      <td>Government Officer</td>\n",
              "      <td>81</td>\n",
              "      <td>97</td>\n",
              "      <td>95</td>\n",
              "      <td>96</td>\n",
              "      <td>65</td>\n",
              "      <td>77</td>\n",
              "      <td>94</td>\n",
              "      <td>605</td>\n",
              "      <td>86.428571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>female</td>\n",
              "      <td>False</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>Artist</td>\n",
              "      <td>71</td>\n",
              "      <td>74</td>\n",
              "      <td>88</td>\n",
              "      <td>80</td>\n",
              "      <td>89</td>\n",
              "      <td>63</td>\n",
              "      <td>86</td>\n",
              "      <td>551</td>\n",
              "      <td>78.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>male</td>\n",
              "      <td>False</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>10</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>84</td>\n",
              "      <td>77</td>\n",
              "      <td>65</td>\n",
              "      <td>65</td>\n",
              "      <td>80</td>\n",
              "      <td>74</td>\n",
              "      <td>76</td>\n",
              "      <td>521</td>\n",
              "      <td>74.428571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender  part_time_job  absence_days  extracurricular_activities  \\\n",
              "0    male          False             3                       False   \n",
              "1  female          False             2                       False   \n",
              "2  female          False             9                        True   \n",
              "3  female          False             5                       False   \n",
              "4    male          False             5                       False   \n",
              "\n",
              "   weekly_self_study_hours   career_aspiration  math_score  history_score  \\\n",
              "0                       27              Lawyer          73             81   \n",
              "1                       47              Doctor          90             86   \n",
              "2                       13  Government Officer          81             97   \n",
              "3                        3              Artist          71             74   \n",
              "4                       10             Unknown          84             77   \n",
              "\n",
              "   physics_score  chemistry_score  biology_score  english_score  \\\n",
              "0             93               97             63             80   \n",
              "1             96              100             90             88   \n",
              "2             95               96             65             77   \n",
              "3             88               80             89             63   \n",
              "4             65               65             80             74   \n",
              "\n",
              "   geography_score  total_score  average_score  \n",
              "0               87          574      82.000000  \n",
              "1               90          640      91.428571  \n",
              "2               94          605      86.428571  \n",
              "3               86          551      78.714286  \n",
              "4               76          521      74.428571  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gja4uzr093Bl"
      },
      "source": [
        "Encoding categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gLUr0mh--khv"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# # Create a LabelEncoder object\n",
        "# label_encoder = LabelEncoder()\n",
        "\n",
        "# # Encode categorical columns using label encoder\n",
        "# df['gender'] = label_encoder.fit_transform(df['gender'])\n",
        "# df['part_time_job'] = label_encoder.fit_transform(df['part_time_job'])\n",
        "# df['extracurricular_activities'] = label_encoder.fit_transform(df['extracurricular_activities'])\n",
        "# df['career_aspiration'] = label_encoder.fit_transform(df['career_aspiration'])\n",
        "# Define mapping dictionaries for categorical features\n",
        "gender_map = {'male': 0, 'female': 1}\n",
        "part_time_job_map = {False: 0, True: 1}\n",
        "extracurricular_activities_map = {False: 0, True: 1}\n",
        "career_aspiration_map = {\n",
        "        'Lawyer': 0, 'Doctor': 1, 'Government Officer': 2, 'Artist': 3, 'Unknown': 4,\n",
        "        'Software Engineer': 5, 'Teacher': 6, 'Business Owner': 7, 'Scientist': 8,\n",
        "        'Banker': 9, 'Writer': 10, 'Accountant': 11, 'Designer': 12,\n",
        "        'Construction Engineer': 13, 'Game Developer': 14, 'Stock Investor': 15,\n",
        "        'Real Estate Developer': 16\n",
        "    }\n",
        "# Apply mapping to the DataFrame\n",
        "df['gender'] = df['gender'].map(gender_map)\n",
        "df['part_time_job'] = df['part_time_job'].map(part_time_job_map)\n",
        "df['extracurricular_activities'] = df['extracurricular_activities'].map(extracurricular_activities_map)\n",
        "df['career_aspiration'] = df['career_aspiration'].map(career_aspiration_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqHO9CGf-uqG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDDSZVjiuvax"
      },
      "source": [
        "Train test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "F8WwgV2Ru2_t"
      },
      "outputs": [],
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train,X_test,y_train,y_test = train_test_split(X_resampled,y_resampled,test_size=0.2, random_state=42)\n",
        "x = df.drop(columns=['average_score', 'total_score'])\n",
        "y = df['average_score']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqMQ8A7eu_Qf",
        "outputId": "72b726ed-7e36-4358-c4a5-c5e7d934dca5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1600, 13)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAytBELAvCFI"
      },
      "source": [
        "Feature Scalling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SxHsjPSVvGXb"
      },
      "outputs": [],
      "source": [
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform both training and testing data\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIlGsmZwvUdL",
        "outputId": "6fc3b5ea-7bf9-4c5a-9fa2-a19eaf6d078f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1600, 13)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train_scaled.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN-WXsnnvYHp"
      },
      "source": [
        "Models Training (Multiple Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OwbAHM0vbO6",
        "outputId": "fbfe15ef-f1c2-4e43-c7ef-de2f99688155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: Logistic Regression\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "==================================================\n",
            "Model: Support Vector Classifier\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "==================================================\n",
            "Model: Random Forest Classifier\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "==================================================\n",
            "Model: K Nearest Neighbors\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "==================================================\n",
            "Model: Decision Tree Classifier\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "==================================================\n",
            "Model: Gaussian Naive Bayes\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "==================================================\n",
            "Model: AdaBoost Classifier\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "==================================================\n",
            "Model: Gradient Boosting Classifier\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "==================================================\n",
            "Model: XGBoost Classifier\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load a sample dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Ensure labels are integers (categorical)\n",
        "y = y.astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Support Vector Classifier\": SVC(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K Nearest Neighbors\": KNeighborsClassifier(),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
        "    \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"XGBoost Classifier\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(\"=\"*50)\n",
        "    print(\"Model:\", name)\n",
        "    # Train the model\n",
        "    model.fit(x_train_scaled, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(x_test_scaled)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Classification Report:\\n\", classification_rep)\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-a-eIhyvkea"
      },
      "source": [
        "Model Selection (Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X575A3PXvoim",
        "outputId": "d5a4c956-fcc3-4016-c863-0e9bb31d5f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  1.0\n",
            "Report:                precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:  [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ],
      "source": [
        "model = RandomForestClassifier()\n",
        "\n",
        "model.fit(x_train_scaled, y_train)\n",
        "# Predict on test set\n",
        "y_pred = model.predict(x_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Accuracy: \",accuracy_score(y_test, y_pred))\n",
        "print(\"Report: \",classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix: \",confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8NYbbk6IoBe",
        "outputId": "f63e0f9a-8505-4dac-e236-c4daf8a3814b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Label: 2\n",
            "Model Prediction: 2\n",
            "Wow! Model doing well.....\n"
          ]
        }
      ],
      "source": [
        "#Test1\n",
        "print(\"Actual Label:\", y_test[10])\n",
        "predicted_label = model.predict(x_test_scaled[10].reshape(1, -1))[0]\n",
        "print(\"Model Prediction:\", predicted_label)\n",
        "\n",
        "if y_test[10] == predicted_label:\n",
        "    print(\"Wow! Model doing well.....\")\n",
        "else:\n",
        "    print(\"Not sure......\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yshLQ3RJI5tI",
        "outputId": "ba0e9f78-a454-4a12-f86d-206453a7a28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Label: 2\n",
            "Model Prediction: 2\n",
            "Wow! Model doing well.....\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming x_train and x_test are already defined and split from your dataset\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Now X_test_scaled is defined and you can use it\n",
        "predicted_label = model.predict(x_test_scaled[10].reshape(1, -1))[0]\n",
        "print(\"Actual Label:\", y_test[10])\n",
        "print(\"Model Prediction:\", predicted_label)\n",
        "\n",
        "if y_test[10] == predicted_label:\n",
        "    print(\"Wow! Model doing well.....\")\n",
        "else:\n",
        "    print(\"Not sure......\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QOBZwpi6Su5",
        "outputId": "6f66614b-9ae3-42ee-91dd-cdb6399e0295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Label: 2\n",
            "Model Prediction: 2\n",
            "Wow! Model doing well.....\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have scaled your test data using StandardScaler or another scaler\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Now X_test_scaled is defined and you can use it\n",
        "predicted_label = model.predict(x_test_scaled[23].reshape(1, -1))[0]\n",
        "print(\"Actual Label:\", y_test[23])\n",
        "print(\"Model Prediction:\", predicted_label)\n",
        "\n",
        "if y_test[23] == predicted_label:\n",
        "    print(\"Wow! Model doing well.....\")\n",
        "else:\n",
        "    print(\"Not sure......\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jhMjK842JCLN"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save the scaler and model objects\n",
        "pickle.dump(scaler, open(\"Models/scaler.pkl\", 'wb'))\n",
        "pickle.dump(model, open(\"Models/model.pkl\", 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5662ki34JUD6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Load the scaler and model\n",
        "scaler = pickle.load(open(\"Models/scaler.pkl\", 'rb'))\n",
        "model = pickle.load(open(\"Models/model.pkl\", 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "stzMZ_e1Ju6D"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load the scaler, label encoder, model, and class names\n",
        "scaler = pickle.load(open(\"Models/scaler.pkl\", 'rb'))\n",
        "model = pickle.load(open(\"Models/model.pkl\", 'rb'))\n",
        "class_names = ['Lawyer', 'Doctor', 'Government Officer', 'Artist', 'Unknown',\n",
        "               'Software Engineer', 'Teacher', 'Business Owner', 'Scientist',\n",
        "               'Banker', 'Writer', 'Accountant', 'Designer',\n",
        "               'Construction Engineer', 'Game Developer', 'Stock Investor',\n",
        "               'Real Estate Developer']\n",
        "\n",
        "def Recommendations(gender, part_time_job, absence_days, extracurricular_activities,\n",
        "                    weekly_self_study_hours, math_score, history_score, physics_score,\n",
        "                    chemistry_score, biology_score, english_score, geography_score,\n",
        "                    total_score,average_score):\n",
        "\n",
        "# Encode categorical variables\n",
        "    gender_encoded = 1 if gender.lower() == 'female' else 0\n",
        "    part_time_job_encoded = 1 if part_time_job else 0\n",
        "    extracurricular_activities_encoded = 1 if extracurricular_activities else 0\n",
        "\n",
        "# Create feature array\n",
        "    feature_array = np.array([[gender_encoded, part_time_job_encoded, absence_days, extracurricular_activities_encoded,\n",
        "                               weekly_self_study_hours, math_score, history_score, physics_score,\n",
        "                               chemistry_score, biology_score, english_score, geography_score,total_score,average_score]])\n",
        "\n",
        "# Scale features\n",
        "    scaled_features = scaler.transform(feature_array)\n",
        "\n",
        "# Predict using the model\n",
        "    probabilities = model.predict_proba(scaled_features)\n",
        "\n",
        "# Get top five predicted classes along with their probabilities\n",
        "    top_classes_idx = np.argsort(-probabilities[0])[:5]\n",
        "    top_classes_names_probs = [(class_names[idx], probabilities[0][idx]) for idx in top_classes_idx]\n",
        "\n",
        "    return top_classes_names_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-faieJzKQ2_",
        "outputId": "f7f12cc7-c477-49fe-c12c-d19089965bd7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 14 features, but StandardScaler is expecting 4 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mRecommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfemale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mpart_time_job\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mabsence_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mextracurricular_activities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mweekly_self_study_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmath_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m65\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mhistory_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mphysics_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m97\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mchemistry_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m94\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbiology_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m71\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43menglish_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m81\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mgeography_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m66\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtotal_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m534\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43maverage_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m76.285714\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop recommended studies with probabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n",
            "Cell \u001b[1;32mIn[18], line 29\u001b[0m, in \u001b[0;36mRecommendations\u001b[1;34m(gender, part_time_job, absence_days, extracurricular_activities, weekly_self_study_hours, math_score, history_score, physics_score, chemistry_score, biology_score, english_score, geography_score, total_score, average_score)\u001b[0m\n\u001b[0;32m     24\u001b[0m     feature_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[gender_encoded, part_time_job_encoded, absence_days, extracurricular_activities_encoded,\n\u001b[0;32m     25\u001b[0m                                weekly_self_study_hours, math_score, history_score, physics_score,\n\u001b[0;32m     26\u001b[0m                                chemistry_score, biology_score, english_score, geography_score,total_score,average_score]])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     scaled_features \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Predict using the model\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(scaled_features)\n",
            "File \u001b[1;32mc:\\Users\\hasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\hasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1006\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1003\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1005\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1006\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
            "File \u001b[1;32mc:\\Users\\hasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\hasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: X has 14 features, but StandardScaler is expecting 4 features as input."
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "final_recommendations = Recommendations(gender='female',\n",
        "                                        part_time_job=False,\n",
        "                                        absence_days=2,\n",
        "                                        extracurricular_activities=False,\n",
        "                                        weekly_self_study_hours=7,\n",
        "                                        math_score=65,\n",
        "                                        history_score=60,\n",
        "                                        physics_score=97,\n",
        "                                        chemistry_score=94,\n",
        "                                        biology_score=71,\n",
        "                                        english_score=81,\n",
        "                                        geography_score=66,\n",
        "                                        total_score=534,\n",
        "                                        average_score=76.285714)\n",
        "\n",
        "print(\"Top recommended studies with probabilities:\")\n",
        "print(\"=\" * 50)\n",
        "for class_name, probability in final_recommendations:\n",
        "    print(f\"{class_name} with probability {probability}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIfRk60AKYkY",
        "outputId": "66a5e9ef-82e9-46e4-8675-77d83a66cd8d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 14 features, but StandardScaler is expecting 4 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage 2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mRecommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfemale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mpart_time_job\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mabsence_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mextracurricular_activities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mweekly_self_study_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmath_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m87\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mhistory_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m73\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mphysics_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m98\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mchemistry_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m91\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbiology_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m79\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43menglish_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mgeography_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m77\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtotal_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m583\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43maverage_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m83.285714\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop recommended studies with probabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
            "Cell \u001b[1;32mIn[18], line 29\u001b[0m, in \u001b[0;36mRecommendations\u001b[1;34m(gender, part_time_job, absence_days, extracurricular_activities, weekly_self_study_hours, math_score, history_score, physics_score, chemistry_score, biology_score, english_score, geography_score, total_score, average_score)\u001b[0m\n\u001b[0;32m     24\u001b[0m     feature_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[gender_encoded, part_time_job_encoded, absence_days, extracurricular_activities_encoded,\n\u001b[0;32m     25\u001b[0m                                weekly_self_study_hours, math_score, history_score, physics_score,\n\u001b[0;32m     26\u001b[0m                                chemistry_score, biology_score, english_score, geography_score,total_score,average_score]])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     scaled_features \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Predict using the model\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(scaled_features)\n",
            "File \u001b[1;32mc:\\Users\\hasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\hasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1006\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1003\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1005\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1006\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
            "File \u001b[1;32mc:\\Users\\hasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\hasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: X has 14 features, but StandardScaler is expecting 4 features as input."
          ]
        }
      ],
      "source": [
        "# Example usage 2\n",
        "final_recommendations = Recommendations(gender='female',\n",
        "                                        part_time_job=False,\n",
        "                                        absence_days=2,\n",
        "                                        extracurricular_activities=False,\n",
        "                                        weekly_self_study_hours=4,\n",
        "                                        math_score=87,\n",
        "                                        history_score=73,\n",
        "                                        physics_score=98,\n",
        "                                        chemistry_score=91,\n",
        "                                        biology_score=79,\n",
        "                                        english_score=60,\n",
        "                                        geography_score=77,\n",
        "                                        total_score=583,\n",
        "                                        average_score=83.285714)\n",
        "\n",
        "print(\"Top recommended studies with probabilities:\")\n",
        "print(\"=\"*50)\n",
        "for class_name, probability in final_recommendations:\n",
        "    print(f\"{class_name} with probability {probability}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxcg3QQEKgny",
        "outputId": "4c480452-260e-4077-d256-9d71240e4faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.3.2\n"
          ]
        }
      ],
      "source": [
        "# sklear version in pychar production\n",
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "# in pycharm env install\n",
        "# pip install scikit-learn==1.3.2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
